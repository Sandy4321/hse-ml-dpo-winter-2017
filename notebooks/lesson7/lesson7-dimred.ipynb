{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print u'Так надо'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Машинное обучение и майнинг данных\n",
    "\n",
    "## 09/03/2017 Методы понижения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Проклятье размерности\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">$d=2$<img src='https://jeremykun.files.wordpress.com/2016/01/2d-distances.png' width=400></th>\n",
    "    <th class=\"tg-031e\">$d=2 \\dots 100$<img src='https://jeremykun.files.wordpress.com/2016/01/distances-animation.gif' width=400></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "$$ \\lim_{d \\rightarrow \\infty} \\frac{\\text{dist}_{max} - \\text{dist}_{min}}{\\text{dist}_{min}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проклятье размености\n",
    "\n",
    "<center><img src='http://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Способы понижения размерности\n",
    "\n",
    "Избавляться от размерности можно методами **отбора признаков (Feature Selection)** и методами **уменьшения размерности (Feature Reduction)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Selection\n",
    "Методы деляться на три группы:\n",
    "* Filter methods \n",
    "    * Признаки рассматриваются независимо друг от друга\n",
    "    * Изучается индивидуальный \"вклад\" призника в предсказываемую переменную\n",
    "    * Быстрое вычисление\n",
    "    * *Пример?*\n",
    "* Wrapper methods\n",
    "    * Идет отбор группы признаков\n",
    "    * Может быть оооочень медленным, но качество, обычно, лучше чем у Filter Methods\n",
    "    * Stepwise feature selection for regression\n",
    "* Embedded methods\n",
    "    * Отбор признаков \"зашит\" в модель\n",
    "    * *Пример?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Filter method - Mutual Information\n",
    "$$MI(y,x) = \\sum_{x,y} p(x,y) \\ln\\left[\\frac{p(x,y)}{p(x)p(y)}\\right]$$\n",
    "Сколько информации $x$ сообщает об $y$.\n",
    "$$NormalizedMI(y,x) = \\frac{MI(y,x)}{H(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Wrapper Methods - Recursive Feature Elimination\n",
    "\n",
    "При данном подходе из линейной модели последовательно удаляются признаки с наименьшим коэффициентом\n",
    "\n",
    "* [AIC](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BA%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BA%D0%B0%D0%B8%D0%BA%D0%B5)\n",
    "* [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "## Метод Главных Компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "* Позволяет уменьшить число переменных, выбрав самые изменчивые из них\n",
    "* Новые переменные являются линейной комбинацией старых переменных\n",
    "* Переход к ортономированному базису\n",
    "\n",
    "<center><img src='http://www.visiondummy.com/wp-content/uploads/2014/05/correlated_2d.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Построение PCA\n",
    "*  Пусть $x \\in \\mathbb{R}^d$ - вектор признаков для какого-то объекта. Будем считать, что $x$ - центрировано и отшкалировано. $E[x_i] = 0, \\sqrt{V[x_i]} = 1, \\quad i=1 \\dots d$\n",
    "* Требуется найти линейное преобразование, которое задается ортогональной матрицей $A$:\n",
    "$$ pc = A^\\top x $$\n",
    "\n",
    "* $pc_i = a_i^\\top x = x^\\top a_i$\n",
    "* $cov[x] = E[(x - E[x])(x - E[x])^\\top] = Exx^\\top = \\Sigma$ -  ковариационная матрица"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Построение PCA\n",
    "\n",
    "* $E[pc_i] = E[a_i^\\top x] = a_i^\\top E[x]$\n",
    "* $cov[pc_i, pc_j] = E[pc_i \\cdot pc_j^\\top] = a_i^\\top \\Sigma a_j $\n",
    "* $\\Sigma$ - симметричная и положительно определенная матрица.\n",
    "    * Собственные числа $\\lambda_i \\in \\mathbb{R}, \\lambda_i \\geq 0$ (Будем считать, что $\\lambda_1 > \\lambda_2  > \\dots > \\lambda_d $\n",
    "    * Собственные вектора при $\\lambda_i \\neq \\lambda_j $ ортогональны: $v_i^\\top v_j = 0$\n",
    "    * У каждого $\\lambda_i$ есть единственный $v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Первая компонента\n",
    "$$ pc_1 = a_1 ^\\top x $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "V[pc_1] = a_1^\\top \\Sigma a_1 \\rightarrow \\max_a \\\\\n",
    "a_1^\\top a_1 = 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "* Строим функцию лагранжа\n",
    "$$ \\mathcal{L}(a_1, \\nu) = a_1^\\top \\Sigma a_1 - \\nu (a_1^\\top a_1 - 1) \\rightarrow max_{a_1, \\nu}$$\n",
    "* Считаем производую по $a_1$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial a_1} = 2\\Sigma a_1 - 2\\nu a_1 = 0 $$\n",
    "* Получается, что $a_1$ один из собственных векторов матрицы $\\Sigma$, причем при $\\lambda_1$\n",
    "$$ V[pc_1] = a_1^\\top \\Sigma a_1 = \\lambda_i a_1^\\top a_1 = \\lambda_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вторая компонента\n",
    "$$ pc_2 = a_2 ^\\top x $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "V[pc_1] = a_2^\\top \\Sigma a_2 \\rightarrow \\max_a \\\\\n",
    "a_2^\\top a_2 = 1 \\\\\n",
    "cov[pc_1, pc_2] = a_2^\\top \\Sigma a_1 = \\lambda_1 a_2^\\top a_1 = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "* Строим функцию лагранжа\n",
    "$$ \\mathcal{L}(a_2, \\nu, \\tau) = a_2^\\top \\Sigma a_2 - \\nu (a_2^\\top a_2 - 1) - \\tau a_2^\\top a_1 \\rightarrow max_{a_1, \\nu}$$\n",
    "\n",
    "Аналогичными выкладками приходим к тому, что $a_2$ - собственный вектор $\\Sigma$ при $\\lambda_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Для любой матрицы $X$ размера $n \\times m$ можно найти разложение вида:\n",
    "$$ X = U S V^\\top ,$$\n",
    "где \n",
    "* $U$ - унитарная матрица, состоящая из собственных векторов $XX^\\top$\n",
    "* $V$ - унитарная матрица, состоящая из собственных векторов $X^\\top X$\n",
    "* $S$ - диагональная матрица с сингулярными числами $s_i = \\sqrt{\\lambda_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD\n",
    "Матрицы $U$ и $V$ ортогональны и могут быть использованы для перехода к ортогональному базису:\n",
    "$$ XV = US $$\n",
    "\n",
    "Сокращение размерности заключается в том, что вместо того, чтобы умножать $X$ на всю матрицу $V$, а лишь на первые $k<m$ её столбцов - матрицу $V'$\n",
    "\n",
    "Квадраты сингулярных чисел в $S$ содержат дисперсию, объясненную в главных компонентах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST\n",
    "<center><img src='https://habrastorage.org/files/cca/f67/7de/ccaf677ded5d4028a3095e9bfbd7fa19.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST PCA\n",
    "\n",
    "<center><img src='http://nikhilbuduma.com/img/autoencoder_digit_exp.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "* PCA понижает размерность признакового пространства\n",
    "* Новые компоненты являются линейной комбинацией исходных признаков\n",
    "* Новые компоненты - ортогональны\n",
    "* Можно применять в моделях и для визуализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# t-SNE\n",
    "## t-distributed stochastic neighbor embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Идея\n",
    "\n",
    "* Перейти в пространство меньшей размерности так, чтобы расстояния между объектами в новом пространстве были подобны расстояниям в исходном пространстве.\n",
    "* Это многомерное шкалирование!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Схожесть между объектами в исходном пространстве $\\mathbb{R}^m$\n",
    "$$\n",
    "p(i, j) = \\frac{p(i | j) + p(j | i)}{2n}, \\quad p(j | i) = \\frac{\\exp(-\\|\\mathbf{x}_j-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}{\\sum_{k \\neq i}\\exp(-\\|\\mathbf{x}_k-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}\n",
    "$$\n",
    "$\\sigma_i$ неявно задается пользователем\n",
    "* Схожесть между объектами в целевом пространстве $\\mathbb{R}^k, k << m$\n",
    "$$\n",
    "q(i, j) = \\frac{g(|\\mathbf{y}_i - \\mathbf{y}_j|)}{\\sum_{k \\neq l} g(|\\mathbf{y}_i - \\mathbf{y}_j|)}\n",
    "$$ \n",
    "где $g(z) = \\frac{1}{1 + z^2}$ - распределение Коши\n",
    "* Критерий\n",
    "$$\n",
    "J_{t-SNE}(y) = KL(P \\| Q) = \\sum_i \\sum_j p(i, j) \\log \\frac{p(i, j)}{q(i, j)} \\rightarrow \\min\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Дивергенция Кульбака-Лейблера\n",
    "\n",
    "* Насколько распределение $P$ отличается от распределения $Q$?\n",
    "$$\n",
    "KL(P \\| Q) = \\sum_z P(z) \\log \\frac{P(z)}{Q(z)}\n",
    "$$\n",
    "\n",
    "<center><img src='http://www.science-emergence.com/media/images/381.png' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Оптимизация\n",
    "\n",
    "* Оптимизируем $J_{t-SNE}(y)$ с помощью градиентного спуска\n",
    "\n",
    "$$\\frac{\\partial J_{t-SNE}}{\\partial y_i}=4 \\sum_j(p(i,j)−q(i,j))(y_i−y_j)g(|y_i−y_j|)$$\n",
    "\n",
    "* [Вывод](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "* [Примеры](http://lvdmaaten.github.io/tsne/)\n",
    "* [Демо](http://distill.pub/2016/misread-tsne/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "973px",
   "left": "0px",
   "right": "1708px",
   "top": "109px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
