{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print u'Так надо'\n",
    "\n",
    "df_auto = pd.read_csv('http://bit.ly/1gIQs6C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Машинное обучение и майнинг данных\n",
    "\n",
    "## 16/02/2017 Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## В предыдущий раз..\n",
    "\n",
    "Мы изучили деревья решений и случайный лес\n",
    "\n",
    "* Немного более интерпретируемая, чем kNN\n",
    "* Минимальная предварительная работа с данными\n",
    "* Автоматический отбор признаков\n",
    "\n",
    "<center><img src='https://camo.githubusercontent.com/c9abdf01ee948c151ddc5e1e2101a483a6c3646c/68747470733a2f2f33312e6d656469612e74756d626c722e636f6d2f37393637306561626539336364643434386331356635626362313938643066622f74756d626c725f696e6c696e655f6e386533393859624b76317330347263332e706e67' width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://i.stack.imgur.com/7oDyK.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Линейные модели\n",
    "\n",
    "* Линейная регрессия\n",
    "* Логистическая регрессия\n",
    "* Метод опорных векторов (Support Vector Machine)\n",
    "\n",
    "\n",
    "* *Обобщенные линейные модели\n",
    "* *LDA (Linear Discriminant Analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_auto.plot(x='mileage', y='price', kind='scatter', s=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Постановка задачи\n",
    "\n",
    "* Дано описание $n$ объектов по $m$ признакам в виде матрицы размера $n \\times m$: $X = [x^{(i)}_j]^{i=1\\dots n}_{j=1\\dots m} $.<br\\> ($x^{(i)}_j$ означает $j$-ый признак $i$-го объекта) <br\\>\n",
    "* Дана **вещественная** зависимая переменная, которая тоже имеет отношение к этим объекам: $y \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Немного формул\n",
    "Наша задача, выявить **линейную** зависимость между признаками в $X$ и значениями в $y$:\n",
    "$$a(X) = \\hat{y} = X\\beta \\quad \\Leftrightarrow \\quad a(x^{(i)}) = \\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots$$\n",
    "Необходимо оценить коэффициенты $\\beta_i$.\n",
    "\n",
    "В случае линейной регрессии коэффициенты $\\beta_i$ рассчитываются так, чтобы минимизировать сумму квадратов ошибок по всем наблюдениям:\n",
    "$$ L(\\beta) = \\frac{1}{2n}(\\hat{y} - y)^{\\top}(\\hat{y} - y) = \\frac{1}{2n}(X\\beta - y)^{\\top}(X\\beta - y) \\rightarrow \\min\\limits_{\\beta} $$ $$ \\Updownarrow $$  $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2  \\rightarrow \\min\\limits_{\\beta_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Можно получить  решение аналитически\n",
    "\n",
    "Пусть $a(x) = \\beta_0 + \\beta_1x_1$\n",
    "\n",
    "Определим функцию потерь, как сумму квадратов разности выдаваемого ответа функционала и реального значения: \n",
    "$$ L(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{i=1}^n(a(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2n}\\sum_{i=1}^n(\\beta_0 + \\beta_1x_1^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "\n",
    "Считаем частные производные по $\\beta_0$, $\\beta_1$, приравниваем к $0$, получаем нужные значения коэффициентов:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)}) = 0$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Можно получить  решение аналитически\n",
    "\n",
    "Можно обобщить на большее количество признаков и перевести в матричный вид:\n",
    "\n",
    "$$ X^\\top X\\beta - X^\\top y = 0 \\quad \\Leftrightarrow \\quad \\beta = (X^\\top X)^{-1} X^\\top y \\quad\\text{(Normal Equation)}$$\n",
    "\n",
    "* [Matrix Calculus](http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/)\n",
    "* [Matrix Cookbook](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Почему так делать не стоит?\n",
    "* Расчет обратной матрицы - дорогая и не совсем устойчивая операция (хотя бы потому что не у всякой матрицы есть обратная)\n",
    "* Лучше будет воспользоваться методами оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "\n",
    "Производная функции $f(x)$ в точке $f(x_0)$:\n",
    "$$ f'(x_0) = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x_0+h) - f(x_0)}{h}$$\n",
    "\n",
    "Производная  равна углу наклона касательной в точке $x_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def deriv_demo(h, x0):\n",
    "\n",
    "    x = np.linspace(0.01, 20, 100)\n",
    "    f = lambda x: np.sin(x)/x\n",
    "\n",
    "    deriv = (f(x0+h) - f(x0))/h\n",
    "\n",
    "    tang = lambda x:  f(x0) + deriv*(x - x0)\n",
    "    \n",
    "    plt.plot(x, f(x), label='$f(x)$')\n",
    "    ylim = plt.ylim()\n",
    "\n",
    "    plt.plot(x,tang(x), label='tangent line (deriv = %f)'%deriv)\n",
    "\n",
    "    plt.scatter(x0, f(x0), marker='s', s=100)\n",
    "    plt.scatter(x0+h, f(x0+h), marker='s', s=100)\n",
    "\n",
    "    plt.ylim(ylim)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(deriv_demo, h=FloatSlider(min=0.0001, max=2, step=0.005), x0=FloatSlider(min=1, max=15, step=.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "\n",
    "* Находим экстремум\n",
    "* Если точка $x_0$ - экстремум функции, и в ней существует производная то $f'(x_0) = 0$\n",
    "* Что если экстремумов много?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Нам повезло, ведь функция ошибки линейной регрессии - выпуклая функция.\n",
    "* Локальный экстремум выпуклой функции = глобальному экстремому"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "\n",
    "* В многомерном случае все обобщается и переходят к производным по направлению и градиенту:\n",
    "$$ f'_v(x_0) = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x_0+vh) - f(x_0)}{h}, \\quad ||v|| = 1 \\quad \\text{Производная по направлению}$$\n",
    "\n",
    "\n",
    "$$ \\frac{ \\partial f(x_1,x_2,\\dots,x_d)}{\\partial x_i} = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x_1, x_2, \\dots, x_i + h, \\dots, x_d) - f(x_1, x_2, \\dots, x_i, \\dots, x_d)}{h} \\quad \\text{Частная производная}$$\n",
    "\n",
    "$$ \\nabla f = \\left(\\frac{\\partial f}{\\partial x_i}\\right),\\quad i=1\\dots d  \\quad \\text{Градиент =  вектор частных производных}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   ## Tiny Refresher\n",
    "\n",
    "* Произвольная по направлению максимальна, если направление совпадает с градиентом.\n",
    "* Градиент — направление наискорейшего роста функции\n",
    "* Антиградиент — направление наискорейшего убывания функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def sq_loss_demo():\n",
    "\n",
    "    beta0 = np.linspace(-5000, 2500, 100)\n",
    "    beta1 = np.linspace(5000, 20000, 100)\n",
    "\n",
    "    x = df_auto.loc[:, ['mileage']]\n",
    "    x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "    X = np.c_[x, np.ones(df_auto.shape[0])]\n",
    "\n",
    "\n",
    "    B0, B1 = np.meshgrid(beta0, beta1)\n",
    "    L = ((X.dot(np.r_[B0.reshape(1,-1), B1.reshape(1,-1)]) - df_auto.loc[:, 'price'].values.reshape(-1,1))**2).sum(axis=0)/(2*df_auto.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.view_init(40, 25)\n",
    "    ax.plot_surface(B0, B1, L.reshape(B0.shape), alpha=0.3,)\n",
    "    # ax.plot_(X, Y, Z)\n",
    "    ax.set_xlabel('beta_0')\n",
    "    ax.set_ylabel('beta_1')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    contour = ax.contour(B0, B1, L.reshape(B0.shape),)\n",
    "    plt.clabel(contour, inline=1, fontsize=10)\n",
    "    ax.set_xlabel('beta_0')\n",
    "    ax.set_ylabel('beta_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "$$ L(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{i=1}^n(\\beta_0 + \\beta_1x_1^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "* Предположим мы выбрали какое-то начальное приближение $(\\hat{\\beta_0}, \\hat{\\beta_1})$\n",
    "* Его можно постараться улучшить - надо двигаться в сторону наискорейшего убывания функции (Антиградиента!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sq_loss_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посчитаем, чему равен градиент функции потерь $L(\\beta_0, \\beta_1):$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)}$$\n",
    "\n",
    "Иногда проще это записать в виде матриц:\n",
    "$$ \\frac{\\partial L}{\\partial \\beta} = X^\\top(X\\beta - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Метод градиентного спуска заключается в итеративном и **одновременном(!!!)** обновлении значений $\\beta$ в направлении, противоположному градиенту:\n",
    "$$ \\beta := \\beta - \\alpha\\frac{\\partial L}{\\partial \\beta}$$\n",
    "\n",
    "* $\\alpha$ -  скорость спуска\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Псевдокод алгоритма\n",
    "\n",
    "* Задаем случайное начальное значение для $\\beta$\n",
    "* Пока не будет достигнуто правило останова:\n",
    "    * Считаем ошибку и значение функции потерь\n",
    "    * Считаем градиент\n",
    "    * Обновляем коэффициенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     37
    ],
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def grad_demo(iters=1, alpha=0.001):\n",
    "    \n",
    "    beta0 = np.linspace(-5000, 2500, 100)\n",
    "    beta1 = np.linspace(5000, 20000, 100)\n",
    "\n",
    "    x = df_auto.loc[:, ['mileage']]\n",
    "    x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "    y = df_auto.loc[:, 'price'].values\n",
    "    X = np.c_[x, np.ones(df_auto.shape[0])]\n",
    "\n",
    "\n",
    "    B0, B1 = np.meshgrid(beta0, beta1)\n",
    "    L = ((X.dot(np.r_[B0.reshape(1,-1), B1.reshape(1,-1)]) - y.reshape(-1,1))**2).sum(axis=0)/(2*df_auto.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.scatter(X[:,0], y)\n",
    "    \n",
    "    Beta, costs, Betas = gradient_descent_upd(X, y, alpha, tol=10**-3, max_iter=iters)\n",
    "    Betas = np.c_[Betas]\n",
    "    \n",
    "    X_1 = np.sort(X, axis=0)\n",
    "    \n",
    "    y_hat = X_1.dot(Betas.T)\n",
    "    \n",
    "    plt.plot(X_1[:,0], y_hat)\n",
    "    \n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    contour = ax.contour(B0, B1, L.reshape(B0.shape),)\n",
    "    plt.clabel(contour, inline=1, fontsize=10)\n",
    "    ax.set_xlabel('beta_0')\n",
    "    ax.set_ylabel('beta_1')\n",
    "    \n",
    "    ax.plot(Betas[:,0], Betas[:, 1], '*-')\n",
    "    \n",
    "\n",
    "def gradient_descent_upd(X, y, alpha, tol=10**-3, max_iter=10):\n",
    "    n = y.shape[0]\n",
    "    Beta = np.array([-3000, 6000])\n",
    "    delta = 10\n",
    "    cost_prev = 0\n",
    "    i = 0\n",
    "    \n",
    "    # for logging\n",
    "    Betas = []\n",
    "    costs = []\n",
    "    \n",
    "    while (delta > tol) and (i <= max_iter):\n",
    "        y_hat = X.dot(Beta)\n",
    "        \n",
    "        # считаем ошибку и значение функции потерь\n",
    "        error = (y_hat - y)\n",
    "        cost = np.sum(error ** 2)/float(2 * n)\n",
    "        delta = abs(cost - cost_prev)\n",
    "        cost_prev = cost\n",
    "        \n",
    "        # считаем градиент\n",
    "        grad = X.T.dot(error) / n\n",
    "\n",
    "        # обновляем коэффициенты\n",
    "        alpha *= 0.95\n",
    "        Beta = Beta - alpha * grad\n",
    "        \n",
    "        # logging\n",
    "        if i % 5 == 0:\n",
    "            costs.append(cost)\n",
    "            Betas.append(Beta)\n",
    "        i += 1\n",
    "        \n",
    "    return Beta, costs, Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grad_demo(iters=105, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Нюансы\n",
    "\n",
    "* Надо подбирать $\\alpha$\n",
    "* Влияние шкал признаков\n",
    "* Попадание в локальный минимум*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вариации градиентного спуска\n",
    "* Стохастический градиентрый спуск (!)\n",
    "* Градиентный спуск с адаптивной скоростью $\\alpha$\n",
    "* Корректировка на импульс\n",
    "* ...\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\"><img src='http://sebastianruder.com/content/images/2016/09/contours_evaluation_optimizers.gif'></th>\n",
    "    <th class=\"tg-031e\"><img src='http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif'></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "[Обзор методов](http://sebastianruder.com/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Так же есть\n",
    "* Методы 0-го порядка\n",
    "    * Покоординатный спуск\n",
    "* Методы 2-го порядка\n",
    "    * Ньютоновские методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Переобучение\\недообучение, мультиколлинеарность и регуляризация\n",
    "\n",
    "<center><img src=http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning_files/Image%20[8].png></center>\n",
    "[Andrew's Ng Machine Learning Class - Stanford]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Мультиколлинеарность** - эффект при котором, подмножество предикторов являются (почти) линейно зависимыми (коэффициент корреляции по модулю близок к 1). Из-за этого:\n",
    "\n",
    "* Матрица $X^{\\top} X$ становится плохо обусловленной или необратимой\n",
    "* Зависимость $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$ перестаёт быть одназначной\n",
    "\n",
    "С этим эффектом можно бороться несколькими способами\n",
    "\n",
    "* Последовательно добавлять переменные в модель\n",
    "* Исключать коррелируемые предикторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Регуляризация\n",
    "\n",
    "В обоих случаях может помочь **регуляризация** - добавление штрафного слагаемого за сложность модели в функцию потерь. В случае линейной регрессии было:\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(a(x^{(i)}) - y^{(i)})^2 $$\n",
    "Стало (Ridge Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(a(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^{m}\\beta_j^2 \\right]$$\n",
    "или (Lasso Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(a(x^{(i)})  - y^{(i)})^2 + \\lambda\\sum_{j=1}^{m}|\\beta_j| \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Природа зависимости\n",
    "\n",
    "Далеко не всегда переменные зависят друг от друга именно в том виде, в котором они даны. Никто не запрещает зависимость вида\n",
    "$$\\log(y) = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\frac{1}{x_1}$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1 x_1^2 + \\beta_2 x_2^2 + \\beta_3 x_1x_2 $$\n",
    "и т.д.\n",
    "\n",
    "Не смотря на то, что могут возникать какие-то нелинейные функции - всё это сводится к **линейной** регрессии (например, о втором пункте, произведите замену $z_1 = \\frac{1}{x_1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demo_weights():\n",
    "    df = pd.read_csv('weights.csv', sep=';', index_col=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    df.plot(x = 'body_w', y='brain_w', kind='scatter', ax=ax[0])\n",
    "    for k, v in df.iterrows():\n",
    "        ax[0].annotate(k, v[:2])\n",
    "    # Должно получится что-то несуразное..\n",
    "    \n",
    "    df['log_body_w'] = np.log(df.body_w)\n",
    "    df['log_brain_w'] = np.log(df.brain_w)\n",
    "    df.plot(x = 'log_body_w', y='log_brain_w', kind='scatter', ax=ax[1])\n",
    "    for k, v in df.iterrows():\n",
    "        ax[1].annotate(k, v[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Логистическая  регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/512px-Svm_separating_hyperplanes_%28SVG%29.svg.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Нам надо найти уравнение прямой (гиперплоскости), которая бы могла разделить два класса ($H_2$ и $H_3$ подходят). В данном случае, уравнение прямой задаётся как: $$g(x) = w_0 + w_1x_1 + w_2x_2 = \\langle w, x \\rangle =  w^\\top x$$\n",
    "\n",
    "* Если $g(x^*) > 0$, то $y^* = \\text{'черный'}$\n",
    "* Если $g(x^*) < 0$, то $y^* = \\text{'белый'}$\n",
    "* Если $g(x^*) = 0$, то мы находимся на линии\n",
    "* т.е. решающее правило: $y^* = sign(g(x^*))$\n",
    "\n",
    "Некоторые геометрические особенности\n",
    "* $\\frac{w_0}{||w||}$ - расстояние от начала координат то прямой\n",
    "* $\\frac{|g(x)|}{||w||}$ - степень \"уверенности\" в классификациий\n",
    "* Величину $M = y\\langle w, x \\rangle = y \\cdot g(x)$ называют **отступом**(margin)\n",
    "\n",
    "Если для какого-то объекта $M \\geq 0$, то его классификация выполнена успешно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Отлично! Значит нам надо просто минимизировать ошибки классификации для всех объектов:\n",
    "\n",
    "$$L(w) = \\sum_i [y^{(i)} \\langle w, x^{(i)} \\rangle < 0] \\rightarrow \\min_w$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Проблема в том, что это будет комбинаторная оптимизация. Существуют различные аппроксимации этой функции ошибок:\n",
    "<center><img src='http://jaquesgrobler.github.io/Online-Scikit-Learn-stat-tut/_images/plot_sgd_loss_functions_11.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def demo_sigmoid():\n",
    "    def sigmoid(z):\n",
    "        return 1./(1. + np.exp(-z))\n",
    "\n",
    "    z = np.linspace(-10, 10, 100)\n",
    "\n",
    "    y = sigmoid(z)\n",
    "    plt.plot(z, y)\n",
    "    plt.xlabel('$z$')\n",
    "    plt.ylabel('$\\sigma(z)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Перед тем как мы начнем, рассмотрим функцию $$\\sigma(z) = \\frac{1}{1 + exp{(-z)}},$$она называется **сигмойда**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "demo_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Можно несколькими способами представить линейную регрессию. Один из самых простых - вот какой.\n",
    "\n",
    "Рассмотрим принадлежность к классу $y=\\pm1$ некого объекта $x$: \n",
    "$$p(y=\\pm1 | x,w)$$ и выразим её через **сигмойду** от **отступа**:\n",
    "\n",
    "$$p(y=\\pm1|x,w) = \\sigma(y \\langle w, x \\rangle) $$\n",
    "\n",
    "А ошибка, которую мы будем минимизировать - логарифмическая:\n",
    "\n",
    "$$L(w) = -\\sum_i \\log(\\sigma(y^{(i)} \\langle w, x^{(i)} \\rangle)) \\rightarrow \\min_w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* С помощью градиентного спуска!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "973px",
   "left": "0px",
   "right": "1708px",
   "top": "109px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "e021f235f0764049b3a9b91854c7be6d": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
